{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### XGBOOST with non-parametric confidence intervals\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import snowflake.connector\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select predictors. We select without UMC_30.\n",
    "\n",
    "predictors = [\"AGE\", \"GENDER\", \n",
    "                \"UMC_HEART\",                 \"UMC_DEMENTIA\" , \n",
    "                \"UMC_CKD\",                   \"UMC_INHERITED_METABOLIC\",   \"UMC_HYP\",\n",
    "                \"UMC_DM2\",                   \"UMC_OBESITY\",               \"UMC_CBD\",\n",
    "                \"UMC_COPD\",                  \"UMC_HYL\",                   \"UMC_ASTHMA\",\n",
    "                \"UMC_CANCER\",                \"UMC_DM1\",                   \"UMC_LIVER\",\n",
    "                \"UMC_PREGNANCY\",             \"UMC_PULM_FIB\",              \"UMC_RHEUMATOID_ARTHRITIS\",\n",
    "                \"UMC_PARKINSONS\",            \"UMC_PANCREATITIS\",         \"UMC_DEV_BEH_DISORDER\",\n",
    "                \"UMC_PROSTATE_CANCER\",       \"UMC_LUNG_CANCER\",           \"UMC_COLORECTAL_CANCER\", \n",
    "                \"UMC_BREAST_CANCER\",         \"UMC_IMMUNE_DEF\",            \"UMC_LYMPHOMA_MYELOMA\",\n",
    "                \"UMC_LUPUS\",                 \"UMC_MULTIPLE_SCLEROSIS\",    \"UMC_IMMUNE_SUPPRESSANTS\",\n",
    "                \"UMC_TRANS\", \"WEEKS_SINCE_2020\"]\n",
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter tuning function\n",
    "\n",
    "def xgboost_hypertuning(training_set):\n",
    "    \n",
    "    # Note: This function follows Kevin Lemagnen's guide (published on the CambridgeSpark blog) closely. Thanks Kevin!\n",
    "    \n",
    "    import time\n",
    "    current_time = time.strftime('%H:%M:%S', time.localtime())\n",
    "    print(current_time)\n",
    "    # Select parameters to tune\n",
    "    params = {\n",
    "        'max_depth':6,\n",
    "        'min_child_weight': 1,\n",
    "        'eta': .1, \n",
    "        'subsample': 1, \n",
    "        'colsample_bytree': 1,\n",
    "        'objective':'binary:logistic',\n",
    "        'gamma': 0\n",
    "    }\n",
    "\n",
    "    # Add eval metric to params\n",
    "    params['eval_metric'] = \"logloss\"\n",
    "\n",
    "    # Set max boosting rounds\n",
    "    num_boost_round = 3000\n",
    "\n",
    "    # Tune max_depth and min_child_weight\n",
    "    gridsearch_params = [\n",
    "        (max_depth, min_child_weight)\n",
    "        for max_depth in range(2, 6)\n",
    "        for min_child_weight in range(4,15)\n",
    "    ]\n",
    "\n",
    "    min_ll = float(\"Inf\")\n",
    "    best_params = None\n",
    "    for max_depth, min_child_weight in gridsearch_params:\n",
    "        print(\"CV with max_depth={}, min_child_weight={}\".format(max_depth, min_child_weight))\n",
    "\n",
    "        # Update parameters\n",
    "        params['max_depth'] = max_depth\n",
    "        params['min_child_weight'] = min_child_weight\n",
    "\n",
    "        # Run CV\n",
    "        cv_results = xgb.cv(\n",
    "        params,\n",
    "        training_set,\n",
    "        num_boost_round=num_boost_round,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics={'logloss'},\n",
    "        early_stopping_rounds=10)\n",
    "\n",
    "        mean_ll = cv_results['test-logloss-mean'].min()\n",
    "        boost_rounds = cv_results['test-logloss-mean'].argmin()\n",
    "        print(\"\\tLogloss {} for {} rounds\".format(mean_ll, boost_rounds))\n",
    "        if mean_ll < min_ll:\n",
    "            min_ll = mean_ll\n",
    "            best_params = (max_depth, min_child_weight)\n",
    "\n",
    "    print(\"Best params: {}, {}, Logloss: {}\".format(best_params[0], best_params[1], min_ll))\n",
    "\n",
    "    current_time = time.strftime('%H:%M:%S', time.localtime())\n",
    "    print(current_time)\n",
    "    # Set these and continue tuning:\n",
    "    params['max_depth'] = best_params[0]\n",
    "    params['min_child_weight'] = best_params[1]\n",
    "\n",
    "    # Tune subsample and colsample_bytree\n",
    "    gridsearch_params = [\n",
    "        (subsample, colsample)\n",
    "        for subsample in [i/10. for i in range(6,11)]\n",
    "        for colsample in [i/10. for i in range(6,11)]\n",
    "    ] \n",
    "\n",
    "    min_ll = float(\"Inf\")\n",
    "    best_params = None\n",
    "    for subsample, colsample in reversed(gridsearch_params):\n",
    "        print(\"CV with subsample={}, colsample={}\".format(subsample, colsample))\n",
    "\n",
    "        # Update parameters\n",
    "        params['subsample'] = subsample\n",
    "        params['colsample_bytree'] = colsample\n",
    "\n",
    "        # Run CV\n",
    "        cv_results = xgb.cv(\n",
    "            params,\n",
    "            training_set,\n",
    "            num_boost_round=num_boost_round,\n",
    "            seed=42,\n",
    "            nfold=5,\n",
    "            metrics={'logloss'},\n",
    "            early_stopping_rounds=10)\n",
    "\n",
    "        mean_ll = cv_results['test-logloss-mean'].min()\n",
    "        boost_rounds = cv_results['test-logloss-mean'].argmin()\n",
    "        print(\"\\tLogloss {} for {} rounds\".format(mean_ll, boost_rounds))\n",
    "        if mean_ll < min_ll:\n",
    "            min_ll = mean_ll\n",
    "            best_params = (subsample, colsample)\n",
    "\n",
    "    print(\"Best params: {}, {}, Logloss: {}\".format(best_params[0], best_params[1], min_ll))\n",
    "\n",
    "    current_time = time.strftime('%H:%M:%S', time.localtime())\n",
    "    print(current_time)\n",
    "    # Set these and continue tuning\n",
    "    params['subsample'] = best_params[0]\n",
    "    params['colsample_bytree'] = best_params[0]\n",
    "\n",
    "    min_ll = float(\"Inf\")\n",
    "    best_params = None\n",
    "    for gamma in [0, .5, 1, 5]:\n",
    "        print(\"CV with gamma={}\".format(gamma))\n",
    "\n",
    "        # Update parameters\n",
    "        params['gamma'] = gamma\n",
    "\n",
    "        # Run CV\n",
    "        cv_results = xgb.cv(\n",
    "            params,\n",
    "            training_set,\n",
    "            num_boost_round=num_boost_round,\n",
    "            seed=42,\n",
    "            nfold=5,\n",
    "            metrics={'logloss'},\n",
    "            early_stopping_rounds=10)\n",
    "\n",
    "        mean_ll = cv_results['test-logloss-mean'].min()\n",
    "        boost_rounds = cv_results['test-logloss-mean'].argmin()\n",
    "        print(\"\\tLogloss {} for {} rounds\".format(mean_ll, boost_rounds))\n",
    "        if mean_ll < min_ll:\n",
    "            min_ll = mean_ll\n",
    "            best_params = gamma\n",
    "\n",
    "    print(\"Best params: {}, Logloss: {}\".format(best_params, min_ll))\n",
    "    \n",
    "    current_time = time.strftime('%H:%M:%S', time.localtime())\n",
    "    print(current_time)\n",
    "    \n",
    "    params['gamma'] = best_params\n",
    "\n",
    "    %time\n",
    "\n",
    "    # Tune ETA\n",
    "    min_ll = float(\"Inf\")\n",
    "    best_params = None\n",
    "\n",
    "    for eta in [.2, .1, .05, .01]:\n",
    "        print(\"CV with eta = {}\".format(eta))\n",
    "\n",
    "        # Update parameter\n",
    "        params['eta'] = eta\n",
    "\n",
    "        # Run and time CV\n",
    "        %time \n",
    "        cv_results = xgb.cv(\n",
    "            params,\n",
    "            training_set,\n",
    "            num_boost_round = num_boost_round,\n",
    "            seed=42,\n",
    "            nfold=5,\n",
    "            metrics=['logloss'],\n",
    "            early_stopping_rounds=10)\n",
    "\n",
    "        # Update best score\n",
    "        mean_ll = cv_results['test-logloss-mean'].min()\n",
    "        boost_rounds = cv_results['test-logloss-mean'].argmin()\n",
    "        print(\"\\tLogloss {} for {} rounds\\n\".format(mean_ll, boost_rounds))\n",
    "        if mean_ll < min_ll:\n",
    "            min_ll = mean_ll\n",
    "            best_params = eta\n",
    "\n",
    "    print(\"Best params: {}, Logloss: {}\".format(best_params, min_ll))\n",
    "\n",
    "    params['eta'] = best_params\n",
    "    current_time = time.strftime('%H:%M:%S', time.localtime())\n",
    "    print(current_time)\n",
    "\n",
    "    # Test on optimal set\n",
    "    #model = xgb.train(params, \n",
    "    #                  training_set,\n",
    "    #                  num_boost_round = num_boost_round,\n",
    "    #                  evals = [(dtest, \"Test\")],\n",
    "    #                  early_stopping_rounds = 10)\n",
    "\n",
    "    #print(\"Best Logloss: {:.2f} in {} rounds\".format(model.best_score, model.best_iteration+1))\n",
    "    print(params)\n",
    "    current_time = time.strftime('%H:%M:%S', time.localtime())\n",
    "    print(current_time)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter tuning on Deaths dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load augmented  dataset:\n",
    "mdf = pd.read_csv('DEBIASED_DATA.csv')\n",
    "\n",
    "# Select date-range:\n",
    "mdf = mdf[(mdf['DAYS_SINCE_2020'] >= 121) & (mdf['DAYS_SINCE_2020'] <= 335)]\n",
    "\n",
    "mdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make model matrix\n",
    "X = mdf\n",
    "Y = X[\"DIED\"]\n",
    "\n",
    "X = X[predictors]\n",
    "\n",
    "# Tuning on 90% model:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = .10, random_state=42)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label = y_train)\n",
    "dtest = xgb.DMatrix(X_test, label = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameters:\n",
    "death_params = xgboost_hypertuning(dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal nrounds based on 90% validation set:\n",
    "model = xgb.train(death_params, \n",
    "                  dtrain,\n",
    "                  num_boost_round = 9999,\n",
    "                  evals = [(dtest, \"Test\")],\n",
    "                  early_stopping_rounds = 10)\n",
    "\n",
    "print(\"Best Logloss: {:.2f} in {} rounds\".format(model.best_score, model.best_iteration+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load augmented  dataset:\n",
    "mdf = pd.read_csv('DEBIASED_DATA_HOSPITALIZATIONS.csv')\n",
    "\n",
    "# Select date-range:\n",
    "mdf = mdf[(mdf['DAYS_SINCE_2020'] >= 121) & (mdf['DAYS_SINCE_2020'] <= 335)]\n",
    "\n",
    "mdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make model matrix\n",
    "X = mdf\n",
    "Y = X[\"HOSPITALIZED\"]\n",
    "\n",
    "X = X[predictors]\n",
    "\n",
    "# Tuning on 90% model:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = .10, random_state=42)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label = y_train)\n",
    "dtest = xgb.DMatrix(X_test, label = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameters:\n",
    "hosp_params = xgboost_hypertuning(dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal nrounds based on 90% validation set:\n",
    "model = xgb.train(hosp_params, \n",
    "                  dtrain,\n",
    "                  num_boost_round = 9999,\n",
    "                  evals = [(dtest, \"Test\")],\n",
    "                  early_stopping_rounds = 10)\n",
    "\n",
    "print(\"Best Logloss: {:.2f} in {} rounds\".format(model.best_score, model.best_iteration+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We next save the model estimate, and 100 bootstrap models (deaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load augmented  dataset:\n",
    "mdf = pd.read_csv('DEBIASED_DATA.csv')\n",
    "\n",
    "# Select date-range:\n",
    "mdf = mdf[(mdf['DAYS_SINCE_2020'] >= 121) & (mdf['DAYS_SINCE_2020'] <= 335)]\n",
    "\n",
    "# Make model matrix\n",
    "X = mdf\n",
    "Y = X[\"DIED\"]\n",
    "X = X[predictors]\n",
    "\n",
    "# Fit with tuned hyperparameters:\n",
    "xgb_fit = xgb.XGBClassifier('binary:logistic', n_estimators = 227,\n",
    "                            max_depth = 3,\n",
    "                            min_child_weight = 12,\n",
    "                            eta = 0.1,\n",
    "                            subsample = 0.9,\n",
    "                            colsample_bytree = 0.9,\n",
    "                            gamma = 1,\n",
    "                            eval_metric = 'logloss'\n",
    "                           )\n",
    "\n",
    "xgb_fit = xgb_fit.fit(X, Y)\n",
    "\n",
    "# Save model\n",
    "file_name = \"XGB_model_estimate.pkl\"\n",
    "pickle.dump(xgb_fit, open(file_name, \"wb\"))\n",
    "\n",
    "print('Model estimate fitted and saved.')\n",
    "\n",
    "# Make confidence intervals by making n models and saving them\n",
    "for i in range(1, 101):\n",
    "    X = mdf.sample(n=len(mdf), replace = True)\n",
    "    Y = X[\"DIED\"]\n",
    "    X = X[predictors]\n",
    "    xgb_fit = xgb.XGBClassifier('binary:logistic', n_estimators = 227,\n",
    "                            max_depth = 3,\n",
    "                            min_child_weight = 12,\n",
    "                            eta = 0.1,\n",
    "                            subsample = 0.9,\n",
    "                            colsample_bytree = 0.9,\n",
    "                            gamma = 1,\n",
    "                            eval_metric = 'logloss'\n",
    "                           )\n",
    "    xgb_fit_ci = xgb_fit.fit(X, Y )\n",
    "    file_name = \"XGB_model_ci_\" + str(i) + \".pkl\"\n",
    "    pickle.dump(xgb_fit_ci, open(file_name, \"wb\"))\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We next save the model estimate, and 100 bootstrap models (hospitalizations)\n",
    "# Load augmented  dataset:\n",
    "mdf = pd.read_csv('DEBIASED_DATA_HOSPITALIZATIONS.csv')\n",
    "\n",
    "# Select date-range:\n",
    "mdf = mdf[(mdf['DAYS_SINCE_2020'] >= 121) & (mdf['DAYS_SINCE_2020'] <= 335)]\n",
    "\n",
    "# Make model matrix\n",
    "X = mdf\n",
    "Y = X[\"HOSPITALIZED\"]\n",
    "X = X[predictors]\n",
    "\n",
    "# Fit with tuned hyperparameters:\n",
    "xgb_fit = xgb.XGBClassifier('binary:logistic', n_estimators = 526,\n",
    "                            max_depth = 5,\n",
    "                            min_child_weight = 10,\n",
    "                            eta = 0.05,\n",
    "                            subsample = 0.8,\n",
    "                            colsample_bytree = 0.8,\n",
    "                            gamma = 5,\n",
    "                            eval_metric = 'logloss'\n",
    "                           )\n",
    "\n",
    "xgb_fit = xgb_fit.fit(X, Y)\n",
    "\n",
    "# Save model\n",
    "file_name = \"XGB_model_hospitalization_estimate.pkl\"\n",
    "pickle.dump(xgb_fit, open(file_name, \"wb\"))\n",
    "\n",
    "print('Model estimate fitted and saved.')\n",
    "\n",
    "for i in range(1, 101):\n",
    "    X = mdf.sample(n=len(mdf), replace = True)\n",
    "    Y = X[\"HOSPITALIZED\"]\n",
    "    X = X[predictors]\n",
    "    xgb_fit_ci = xgb.XGBClassifier('binary:logistic', n_estimators = 526,\n",
    "                            max_depth = 5,\n",
    "                            min_child_weight = 10,\n",
    "                            eta = 0.05,\n",
    "                            subsample = 0.8,\n",
    "                            colsample_bytree = 0.8,\n",
    "                            gamma = 5,\n",
    "                            eval_metric = 'logloss'\n",
    "                           )\n",
    "    xgb_fit_ci = xgb_fit.fit(X, Y )\n",
    "    file_name = \"XGB_model_hospitalization_ci_\" + str(i) + \".pkl\"\n",
    "    pickle.dump(xgb_fit_ci, open(file_name, \"wb\"))\n",
    "    print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then make predictions for a random sample from the 103 million, for both hospitalizations and deaths, and export a csv:\n",
    "rep_sample = pd.read_csv(r'representative_sample_from_full_data.csv')\n",
    "rep_sample['WEEKS_SINCE_2020'] = 48\n",
    "\n",
    "# Load death model:\n",
    "death_model = pickle.load(open('XGB_model_estimate.pkl', 'rb'))\n",
    "rep_sample['DEATH_PREDICTION'] = pd.DataFrame(death_model.predict_proba(rep_sample[predictors])[:, 1])\n",
    "\n",
    "# Load hospitalization model:\n",
    "hosp_model = pickle.load(open('XGB_model_hospitalization_estimate.pkl', 'rb'))\n",
    "rep_sample['HOSPITALIZATION_PREDICTION'] = pd.DataFrame(hosp_model.predict_proba(rep_sample[predictors])[:, 1])\n",
    "\n",
    "# Save:\n",
    "rep_sample = rep_sample[['AGE', 'GENDER', \"UMC_30\", 'DEATH_PREDICTION', 'HOSPITALIZATION_PREDICTION']]\n",
    "rep_sample.to_csv(r'representative_sample_narrow.csv')\n",
    "rep_sample.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
